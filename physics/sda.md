---
layout: post
title: 统计物理
permalink: /physics/sda
usemathjax: true
---

# 第一章 随机变量

### 累积分布函数

观测值小于等于x的概率

---

$$
\begin{align}
	F(x) &= \sum_{xi \le x} P(x_i) & \text{离散}
	\\
	F(x) &= \int_{-\infin}^x f(x\prime)dx\prime & \text{连续}
\end{align}
$$

---



### 分位数

使$F(x) = \alpha$的x，或者说$x = F^{-1}(\alpha)$



### 联合概率密度分布函数

---

$$
\iiint f(x, y) dx dy = 1
$$

---

$$
f_x(x) = \int f(x, y) dy = \int g(x | y)f_y(y) dy
$$

---

上面这个函数称为边缘概率密度函数，显然这个函数在有两个变量的情况下只考虑一个变量



重要公式（1.32）a(x)为x的函数，x的分布知道，为f(x)，求a的分布g(a)

---

$$
g(a) = f(x(a))\left|\frac{dx}{da}\right|
$$

---

> 备注：如果函数不是单调函数，那么需要把所有反函数x(a)带进去，求完后加起来
>
> 下面这个公式是对他的推广

![image-20250617130106758](./sda/img/image-20250617130106758.png)



1.35梅林卷集，只适用于z=xy的情况求z的概率密度函数

![image-20250617125839950](./sda/img/image-20250617125839950.png)

下面只适用于z=x+y

![image-20250617125953089](./sda/img/image-20250617125953089.png)



### 方差、协方差、关联系数

![image-20250617130247004](./sda/img/image-20250617130247004.png)

![image-20250617130259011](./sda/img/image-20250617130259011.png)

![image-20250617130313512](./sda/img/image-20250617130313512.png)

![image-20250617130500939](./sda/img/image-20250617130500939.png)



## 1.6 误差传递

![image-20250617134843714](./sda/img/image-20250617134843714.png)

![image-20250617140455063](./sda/img/image-20250617140455063.png)



# 第二章 常用分布

## 2.1 二项/多项分布

要么成功，要么失败

N次试验中成功n次概率、期待值、方差

---

$$
\begin{align}
	f(n; N, p) &= \mathrm{C}^n_{N} p^n (1-p)^{N-n}
	\\
	E[n] &= Np
	\\
	V[n] &= Np(1 - p)
\end{align}
$$

---

多项分布，每一次实验的结果不止两个，每一个结果对应一个p，显然

---

$$
\sum_i p_i = 1
$$

---

若有N次实验，每一次试验产生一个结果，试验完成后，第一个可能结果有n1次出现，第二个有n2个，显然

---

$$
\begin{align}
	\sum_i n_i &= N
	\\
	f(n_1, ..., n_m; N; p_1, ..., p_m) &= \frac{N!}{n_1!n_2!...n_m!}p_1^{n_1}...p_m^{n_m} 
\end{align}
$$

---

如果这样思考：第i个事件发生了即为成功，没有发生（发生了其他的事件）则为失败，这不就是个二项分布吗，所以第i个事件的期望和方差同上面的二项分布



## 2.2 泊松分布

在二项分布的极限情况，即N很大p很小，Np为一个有限值，二项分布近似为泊松分布

---

$$
\begin{align}
	f(n;\nu) &= \frac{\nu^n}{n!} e^{-\nu}
	\\
	E[n] &= \nu
	\\
	V[n] &= \nu
\end{align}
$$

---

**若$\nu$很大，则泊松分布可以近似为高斯分布**



## 2.3 均匀分布

---

$$
\begin{align}
	f(x; \alpha, \beta) &= \begin{cases} \frac{1}{\beta - \alpha} & \alpha \le x \le \beta 
	\\ 0 & otherwise
	\end{cases}
	
	\\
	E[x] &= \frac{1}{2} (\alpha + \beta)
    \\
    V[x] &= \frac{1}{12} (\beta - \alpha)^2
\end{align}
$$

---



## 2.4 指数分布

---

$$
\begin{align}
	f(x; \alpha, \beta) &= \frac{1}{\xi}e^{-x / \xi}
	\\
	E[x] &= \xi
	\\
	V[x] &= \xi^2
\end{align}
$$

---



## 2.5 高斯分布

---

$$
\begin{align}
	f(x; \mu, \sigma^2) &= \frac{1}{\sqrt{2 \pi \sigma^2}}\exp(-\frac{(x - \mu)^2}{2 \sigma^2})
	\\
	E[x] &= \mu
	\\
	V[x] &= \sigma^2
\end{align}
$$

---

一个小定理:

---

$$
y \sim N(\mu, \sigma^2)
\Rightarrow
x = \frac{y - \mu}{\sigma} \sim N(0, 1)
$$

---

### 中心极限定理

如果n个**独立连续**随机变量，$\mu_i, \sigma^2_i$，**没有说他们是什么分布**，他们的和满足$\mu = \sum_i \mu_i, \sigma^2 = \sum_i \sigma^2_i$的高斯分布。



## 2.6 卡方分布

函数形式不想写

![image-20250617154700634](./sda/img/image-20250617154700634.png)

---

$$
\begin{align}
	E[z] &= n
	\\
	V[z] &= 2n
\end{align}
$$

---

上面的n是自由度，若$x_i \sim N(0, 1)$，则$\sum_ix_i^2$满足自由度为N的卡方分布。所以拟合的卡方每自由度最好是1



# 第三章 蒙卡方法

## 3.2 变量变换法

想要用f(x)抽样x，

1. 求f(x)的累积分布
2. 求累积分布的逆函数
3. 把均匀分布带入逆函数，即可



# 第四章 统计检验

## 4.1 假设、检验统计量、显著水平和效力

统计检验目的：看看观测的数据和预期的假设对不对的上

- 零假设：考察中的假设
- 备选假设：和零假设比较的
- 简单假设：在这个假设下，概率密度函数可以直接被完全确定
- 复杂假设：在这个假设下确定的概率密度函数有个可以变动的参数



- 检验统计量，完全可以用原始数据来当作检验统计量，但是维度太高了，所以换一个简单的
  - 可以这么理解$t = t(\vec x)$
  - 这也是个统计量，**从x算出来的或者测出来的**，你不用管这个啰嗦奇葩的术语，真啰嗦啊生怕我看懂它？
  - 在假设不同的情况下，统计量的分布也不同，看下面这个b图，这个是在假设正确的情况下画出来的，也就是说真实的t分布可能不是这个样子的

![image-20250617184929190](./sda/img/image-20250617184929190.png)

- 拒绝域：
  - 你看上面这个b图，如果H0正确的话，那么测出来的t处于2.7以上的位置是不是几率特别小？
  - 可以把2.7及以上的位置定义为拒绝域
  - 按照我们的假设H0，处在这个拒绝域概率很小，假设这个概率为5%吧！
  - 这个5%就叫做**显著水平**
  - 2.7这个位置就叫做tCut，**判选条件**
  - 现在来看，如果H0为真，那是不是有5%拒绝H0？，这个叫第一类误差

![image-20250617191557392](./sda/img/image-20250617191557392.png)

- 效力
  - 现在需要看g(t|H1)了，如果H1是真的，那么t测量出来比2.7小是不是概率很低？那么2.7就是tCut了，假设这个概率为7%，这个7%为显著水平
  - 现在我们看，我们先入为主，把H0认为零假设，测出来的t也让我们接受H0,但是这个时候H1是真的，此时这个7%称为第二类误差，也就是下面这个图的beta，同时可以定义效力

![image-20250617192521252](./sda/img/image-20250617192521252.png)

> 可以看出，我们的零假设不同，这些alpha和beta的值也不同



## 4.2 粒子选择的统计检验

在4.1中，把H0认为是电子，H1认为是pi，认为电子是目标，pi是背景。那么

**选择效率**

-  $\epsilon_e = 1 - \alpha$
- $\epsilon_\pi = \beta$

> 若pi是目标，那么
>
>  $\epsilon_e = \alpha$
>
> $\epsilon_\pi = 1 - \beta$
